{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d0667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conllu in ./.venv/lib/python3.12/site-packages (6.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e3e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2298896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MRC Psycholinguistic Database...\n",
      "Total valid words: 150000\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIR = \"./bert_mrc_embeddings\"\n",
    "os.makedirs(EMBEDDING_DIR, exist_ok=True)\n",
    "\n",
    "# loading dataset\n",
    "print(\"Loading MRC Psycholinguistic Database...\")\n",
    "mrc_dataset = load_dataset(\"StephanAkkerman/MRC-psycholinguistic-database\")['train']\n",
    "\n",
    "# extracting features \n",
    "words = mrc_dataset['Word']\n",
    "img = np.array(mrc_dataset['Imageability'], dtype=float)\n",
    "conc = np.array(mrc_dataset['Concreteness'], dtype=float)\n",
    "nsyl = np.array(mrc_dataset['Number of Syllables'], dtype=float)\n",
    "\n",
    "valid = ~np.isnan(img) & ~np.isnan(conc) & ~np.isnan(nsyl)\n",
    "words = [words[i] for i in range(len(words)) if valid[i]]\n",
    "img = img[valid]\n",
    "conc = conc[valid]\n",
    "nsyl = nsyl[valid]\n",
    "\n",
    "print(f\"Total valid words: {len(words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5767619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading BERT...\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True).eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a0bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_word_embedding_per_layer(word, model, tokenizer, device):\n",
    "    try:\n",
    "        encoded = tokenizer(word, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "        hidden_states = output.hidden_states\n",
    "        return [layer[0, 0, :].cpu().numpy() for layer in hidden_states]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcfb846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_set(word_list, label_list, desc):\n",
    "    all_embeddings = [[] for _ in range(13)]\n",
    "    all_labels = []\n",
    "    for i in tqdm(range(len(word_list)), desc=f\"{desc} Embeddings\"):\n",
    "        emb = get_single_word_embedding_per_layer(word_list[i], model, tokenizer, device)\n",
    "        if emb is None:\n",
    "            continue\n",
    "        for layer in range(13):\n",
    "            all_embeddings[layer].append(emb[layer])\n",
    "        all_labels.append(label_list[i])\n",
    "    final = [torch.tensor(np.vstack(all_embeddings[layer]), dtype=torch.float32) for layer in range(13)]\n",
    "    return final, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5106f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probing_cv(words, labels, feature_name):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    results = {f\"layer_{i}\": {'mae': [], 'r2': []} for i in range(13)}\n",
    "\n",
    "    for fold, (train_val_idx, test_idx) in enumerate(kf.split(words)):\n",
    "        print(f\"\\nFold {fold+1}/5 - {feature_name}\")\n",
    "\n",
    "        # splitting data\n",
    "        train_val_words = [words[i] for i in train_val_idx]\n",
    "        train_val_labels = [labels[i] for i in train_val_idx]\n",
    "        test_words = [words[i] for i in test_idx]\n",
    "        test_labels = [labels[i] for i in test_idx]\n",
    "\n",
    "        # splitting train/val\n",
    "        t_idx, d_idx = train_test_split(np.arange(len(train_val_words)), test_size=0.2, random_state=42)\n",
    "        train_words = [train_val_words[i] for i in t_idx]\n",
    "        dev_words = [train_val_words[i] for i in d_idx]\n",
    "        train_labels = [train_val_labels[i] for i in t_idx]\n",
    "        dev_labels = [train_val_labels[i] for i in d_idx]\n",
    "\n",
    "        # getting embeddings\n",
    "        train_embs, train_labels = get_embeddings_for_set(train_words, train_labels, \"Train\")\n",
    "        test_embs, test_labels = get_embeddings_for_set(test_words, test_labels, \"Test\")\n",
    "\n",
    "        # training and evaluating on each layer\n",
    "        for layer in range(13):\n",
    "            print(f\"  ➤ Layer {layer}\")\n",
    "            X_train = train_embs[layer].numpy()\n",
    "            X_test = test_embs[layer].numpy()\n",
    "            y_train = train_labels\n",
    "            y_test = test_labels\n",
    "\n",
    "            if len(np.unique(y_train)) < 2:\n",
    "                results[f\"layer_{layer}\"]['mae'].append(float('inf'))\n",
    "                results[f\"layer_{layer}\"]['r2'].append(0.0)\n",
    "                continue\n",
    "\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(X_train, y_train)\n",
    "            preds = reg.predict(X_test)\n",
    "            mae = mean_absolute_error(y_test, preds)\n",
    "            r2 = r2_score(y_test, preds)\n",
    "            results[f\"layer_{layer}\"]['mae'].append(mae)\n",
    "            results[f\"layer_{layer}\"]['r2'].append(r2)\n",
    "            print(f\"     MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a3c44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5 - Imageability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [08:00<00:00, 199.73it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:30<00:00, 198.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 51.2839, R²: -0.0001\n",
      "  ➤ Layer 1\n",
      "     MAE: 54.2580, R²: 0.1907\n",
      "  ➤ Layer 2\n",
      "     MAE: 53.7592, R²: 0.2194\n",
      "  ➤ Layer 3\n",
      "     MAE: 53.8679, R²: 0.2430\n",
      "  ➤ Layer 4\n",
      "     MAE: 53.5519, R²: 0.2652\n",
      "  ➤ Layer 5\n",
      "     MAE: 51.5576, R²: 0.3099\n",
      "  ➤ Layer 6\n",
      "     MAE: 51.9300, R²: 0.3058\n",
      "  ➤ Layer 7\n",
      "     MAE: 51.6158, R²: 0.3151\n",
      "  ➤ Layer 8\n",
      "     MAE: 50.8079, R²: 0.3320\n",
      "  ➤ Layer 9\n",
      "     MAE: 50.3583, R²: 0.3353\n",
      "  ➤ Layer 10\n",
      "     MAE: 50.9663, R²: 0.3258\n",
      "  ➤ Layer 11\n",
      "     MAE: 51.6647, R²: 0.3185\n",
      "  ➤ Layer 12\n",
      "     MAE: 55.7515, R²: 0.2335\n",
      "\n",
      "Fold 2/5 - Imageability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [08:19<00:00, 192.25it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [03:25<00:00, 146.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 52.0825, R²: -0.0000\n",
      "  ➤ Layer 1\n",
      "     MAE: 54.7867, R²: 0.2037\n",
      "  ➤ Layer 2\n",
      "     MAE: 54.1992, R²: 0.2336\n",
      "  ➤ Layer 3\n",
      "     MAE: 54.2255, R²: 0.2591\n",
      "  ➤ Layer 4\n",
      "     MAE: 54.0932, R²: 0.2809\n",
      "  ➤ Layer 5\n",
      "     MAE: 52.5136, R²: 0.3229\n",
      "  ➤ Layer 6\n",
      "     MAE: 52.7665, R²: 0.3192\n",
      "  ➤ Layer 7\n",
      "     MAE: 51.9924, R²: 0.3374\n",
      "  ➤ Layer 8\n",
      "     MAE: 51.4204, R²: 0.3447\n",
      "  ➤ Layer 9\n",
      "     MAE: 50.4325, R²: 0.3559\n",
      "  ➤ Layer 10\n",
      "     MAE: 50.9046, R²: 0.3493\n",
      "  ➤ Layer 11\n",
      "     MAE: 51.5964, R²: 0.3386\n",
      "  ➤ Layer 12\n",
      "     MAE: 55.6332, R²: 0.2514\n",
      "\n",
      "Fold 3/5 - Imageability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [07:55<00:00, 201.80it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:30<00:00, 199.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 52.1242, R²: -0.0000\n",
      "  ➤ Layer 1\n",
      "     MAE: 54.9681, R²: 0.1952\n",
      "  ➤ Layer 2\n",
      "     MAE: 54.7848, R²: 0.2184\n",
      "  ➤ Layer 3\n",
      "     MAE: 54.4095, R²: 0.2446\n",
      "  ➤ Layer 4\n",
      "     MAE: 54.3027, R²: 0.2658\n",
      "  ➤ Layer 5\n",
      "     MAE: 52.5310, R²: 0.3116\n",
      "  ➤ Layer 6\n",
      "     MAE: 52.5120, R²: 0.3113\n",
      "  ➤ Layer 7\n",
      "     MAE: 52.1284, R²: 0.3180\n",
      "  ➤ Layer 8\n",
      "     MAE: 51.7424, R²: 0.3237\n",
      "  ➤ Layer 9\n",
      "     MAE: 50.9838, R²: 0.3371\n",
      "  ➤ Layer 10\n",
      "     MAE: 51.1983, R²: 0.3321\n",
      "  ➤ Layer 11\n",
      "     MAE: 51.9540, R²: 0.3223\n",
      "  ➤ Layer 12\n",
      "     MAE: 56.0743, R²: 0.2385\n",
      "\n",
      "Fold 4/5 - Imageability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [07:58<00:00, 200.48it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:29<00:00, 200.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 51.4620, R²: -0.0000\n",
      "  ➤ Layer 1\n",
      "     MAE: 54.1330, R²: 0.2010\n",
      "  ➤ Layer 2\n",
      "     MAE: 53.9678, R²: 0.2242\n",
      "  ➤ Layer 3\n",
      "     MAE: 53.6938, R²: 0.2545\n",
      "  ➤ Layer 4\n",
      "     MAE: 53.1925, R²: 0.2813\n",
      "  ➤ Layer 5\n",
      "     MAE: 51.7397, R²: 0.3167\n",
      "  ➤ Layer 6\n",
      "     MAE: 51.7697, R²: 0.3150\n",
      "  ➤ Layer 7\n",
      "     MAE: 51.6010, R²: 0.3231\n",
      "  ➤ Layer 8\n",
      "     MAE: 50.7643, R²: 0.3383\n",
      "  ➤ Layer 9\n",
      "     MAE: 50.0000, R²: 0.3483\n",
      "  ➤ Layer 10\n",
      "     MAE: 50.6962, R²: 0.3356\n",
      "  ➤ Layer 11\n",
      "     MAE: 51.6485, R²: 0.3247\n",
      "  ➤ Layer 12\n",
      "     MAE: 55.5617, R²: 0.2410\n",
      "\n",
      "Fold 5/5 - Imageability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [08:00<00:00, 199.62it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:29<00:00, 200.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 51.9495, R²: -0.0000\n",
      "  ➤ Layer 1\n",
      "     MAE: 54.7232, R²: 0.1976\n",
      "  ➤ Layer 2\n",
      "     MAE: 54.1514, R²: 0.2302\n",
      "  ➤ Layer 3\n",
      "     MAE: 53.9971, R²: 0.2546\n",
      "  ➤ Layer 4\n",
      "     MAE: 53.8138, R²: 0.2781\n",
      "  ➤ Layer 5\n",
      "     MAE: 52.4509, R²: 0.3159\n",
      "  ➤ Layer 6\n",
      "     MAE: 52.2145, R²: 0.3221\n",
      "  ➤ Layer 7\n",
      "     MAE: 51.8911, R²: 0.3329\n",
      "  ➤ Layer 8\n",
      "     MAE: 51.1995, R²: 0.3480\n",
      "  ➤ Layer 9\n",
      "     MAE: 50.7933, R²: 0.3541\n",
      "  ➤ Layer 10\n",
      "     MAE: 51.1169, R²: 0.3476\n",
      "  ➤ Layer 11\n",
      "     MAE: 51.8209, R²: 0.3375\n",
      "  ➤ Layer 12\n",
      "     MAE: 55.8348, R²: 0.2505\n"
     ]
    }
   ],
   "source": [
    "results_img = probing_cv(words, img.tolist(), \"Imageability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57bb1b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5 - Concreteness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [07:58<00:00, 200.50it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:31<00:00, 198.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 44.8743, R²: -0.0001\n",
      "  ➤ Layer 1\n",
      "     MAE: 48.8615, R²: 0.1717\n",
      "  ➤ Layer 2\n",
      "     MAE: 48.5401, R²: 0.1977\n",
      "  ➤ Layer 3\n",
      "     MAE: 48.7515, R²: 0.2208\n",
      "  ➤ Layer 4\n",
      "     MAE: 48.4890, R²: 0.2457\n",
      "  ➤ Layer 5\n",
      "     MAE: 47.0873, R²: 0.2893\n",
      "  ➤ Layer 6\n",
      "     MAE: 47.3708, R²: 0.2863\n",
      "  ➤ Layer 7\n",
      "     MAE: 47.0740, R²: 0.2964\n",
      "  ➤ Layer 8\n",
      "     MAE: 46.2736, R²: 0.3155\n",
      "  ➤ Layer 9\n",
      "     MAE: 45.9339, R²: 0.3198\n",
      "  ➤ Layer 10\n",
      "     MAE: 46.6175, R²: 0.3085\n",
      "  ➤ Layer 11\n",
      "     MAE: 46.9910, R²: 0.3030\n",
      "  ➤ Layer 12\n",
      "     MAE: 50.2884, R²: 0.2194\n",
      "\n",
      "Fold 2/5 - Concreteness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [07:59<00:00, 200.26it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:30<00:00, 199.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 45.2801, R²: -0.0000\n",
      "  ➤ Layer 1\n",
      "     MAE: 49.1835, R²: 0.1808\n",
      "  ➤ Layer 2\n",
      "     MAE: 48.8234, R²: 0.2078\n",
      "  ➤ Layer 3\n",
      "     MAE: 49.0018, R²: 0.2344\n",
      "  ➤ Layer 4\n",
      "     MAE: 48.8974, R²: 0.2583\n",
      "  ➤ Layer 5\n",
      "     MAE: 47.8924, R²: 0.2953\n",
      "  ➤ Layer 6\n",
      "     MAE: 48.0836, R²: 0.2945\n",
      "  ➤ Layer 7\n",
      "     MAE: 47.2897, R²: 0.3149\n",
      "  ➤ Layer 8\n",
      "     MAE: 46.6379, R²: 0.3251\n",
      "  ➤ Layer 9\n",
      "     MAE: 45.8310, R²: 0.3356\n",
      "  ➤ Layer 10\n",
      "     MAE: 46.3436, R²: 0.3286\n",
      "  ➤ Layer 11\n",
      "     MAE: 46.8932, R²: 0.3170\n",
      "  ➤ Layer 12\n",
      "     MAE: 50.1811, R²: 0.2304\n",
      "\n",
      "Fold 3/5 - Concreteness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [08:01<00:00, 199.57it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:31<00:00, 198.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 45.7154, R²: -0.0000\n",
      "  ➤ Layer 1\n",
      "     MAE: 49.5775, R²: 0.1781\n",
      "  ➤ Layer 2\n",
      "     MAE: 49.5344, R²: 0.1988\n",
      "  ➤ Layer 3\n",
      "     MAE: 49.4169, R²: 0.2230\n",
      "  ➤ Layer 4\n",
      "     MAE: 49.4427, R²: 0.2459\n",
      "  ➤ Layer 5\n",
      "     MAE: 48.0861, R²: 0.2909\n",
      "  ➤ Layer 6\n",
      "     MAE: 47.9715, R²: 0.2927\n",
      "  ➤ Layer 7\n",
      "     MAE: 47.5627, R²: 0.3016\n",
      "  ➤ Layer 8\n",
      "     MAE: 47.2993, R²: 0.3078\n",
      "  ➤ Layer 9\n",
      "     MAE: 46.6455, R²: 0.3201\n",
      "  ➤ Layer 10\n",
      "     MAE: 46.8747, R²: 0.3147\n",
      "  ➤ Layer 11\n",
      "     MAE: 47.4897, R²: 0.3054\n",
      "  ➤ Layer 12\n",
      "     MAE: 50.6895, R²: 0.2233\n",
      "\n",
      "Fold 4/5 - Concreteness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [08:01<00:00, 199.39it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:31<00:00, 198.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 44.8070, R²: -0.0000\n",
      "  ➤ Layer 1\n",
      "     MAE: 48.7706, R²: 0.1781\n",
      "  ➤ Layer 2\n",
      "     MAE: 48.7825, R²: 0.1982\n",
      "  ➤ Layer 3\n",
      "     MAE: 48.6742, R²: 0.2283\n",
      "  ➤ Layer 4\n",
      "     MAE: 48.2014, R²: 0.2601\n",
      "  ➤ Layer 5\n",
      "     MAE: 47.1832, R²: 0.2930\n",
      "  ➤ Layer 6\n",
      "     MAE: 47.3622, R²: 0.2905\n",
      "  ➤ Layer 7\n",
      "     MAE: 47.0306, R²: 0.3038\n",
      "  ➤ Layer 8\n",
      "     MAE: 46.1932, R²: 0.3207\n",
      "  ➤ Layer 9\n",
      "     MAE: 45.6500, R²: 0.3305\n",
      "  ➤ Layer 10\n",
      "     MAE: 46.3131, R²: 0.3171\n",
      "  ➤ Layer 11\n",
      "     MAE: 47.0491, R²: 0.3072\n",
      "  ➤ Layer 12\n",
      "     MAE: 50.0741, R²: 0.2219\n",
      "\n",
      "Fold 5/5 - Concreteness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [08:01<00:00, 199.57it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:30<00:00, 199.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 45.5360, R²: -0.0000\n",
      "  ➤ Layer 1\n",
      "     MAE: 49.4075, R²: 0.1778\n",
      "  ➤ Layer 2\n",
      "     MAE: 49.1445, R²: 0.2053\n",
      "  ➤ Layer 3\n",
      "     MAE: 49.1125, R²: 0.2308\n",
      "  ➤ Layer 4\n",
      "     MAE: 48.8947, R²: 0.2580\n",
      "  ➤ Layer 5\n",
      "     MAE: 47.9237, R²: 0.2955\n",
      "  ➤ Layer 6\n",
      "     MAE: 47.7258, R²: 0.3013\n",
      "  ➤ Layer 7\n",
      "     MAE: 47.4137, R²: 0.3145\n",
      "  ➤ Layer 8\n",
      "     MAE: 46.8062, R²: 0.3288\n",
      "  ➤ Layer 9\n",
      "     MAE: 46.5253, R²: 0.3362\n",
      "  ➤ Layer 10\n",
      "     MAE: 46.7952, R²: 0.3298\n",
      "  ➤ Layer 11\n",
      "     MAE: 47.4056, R²: 0.3195\n",
      "  ➤ Layer 12\n",
      "     MAE: 50.5348, R²: 0.2347\n"
     ]
    }
   ],
   "source": [
    "results_conc = probing_cv(words, conc.tolist(), \"Concreteness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "793a5e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5 - Number of Syllables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [07:57<00:00, 201.16it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:29<00:00, 201.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 1.3677, R²: -0.0000\n",
      "  ➤ Layer 1\n",
      "     MAE: 0.8720, R²: 0.4794\n",
      "  ➤ Layer 2\n",
      "     MAE: 0.8246, R²: 0.5294\n",
      "  ➤ Layer 3\n",
      "     MAE: 0.8106, R²: 0.5435\n",
      "  ➤ Layer 4\n",
      "     MAE: 0.7624, R²: 0.5888\n",
      "  ➤ Layer 5\n",
      "     MAE: 0.7538, R²: 0.5973\n",
      "  ➤ Layer 6\n",
      "     MAE: 0.7157, R²: 0.6298\n",
      "  ➤ Layer 7\n",
      "     MAE: 0.7085, R²: 0.6376\n",
      "  ➤ Layer 8\n",
      "     MAE: 0.7059, R²: 0.6420\n",
      "  ➤ Layer 9\n",
      "     MAE: 0.7081, R²: 0.6413\n",
      "  ➤ Layer 10\n",
      "     MAE: 0.7146, R²: 0.6343\n",
      "  ➤ Layer 11\n",
      "     MAE: 0.7354, R²: 0.6178\n",
      "  ➤ Layer 12\n",
      "     MAE: 0.7668, R²: 0.5954\n",
      "\n",
      "Fold 2/5 - Number of Syllables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [07:54<00:00, 202.32it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:29<00:00, 201.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 1.3650, R²: -0.0000\n",
      "  ➤ Layer 1\n",
      "     MAE: 0.8758, R²: 0.4744\n",
      "  ➤ Layer 2\n",
      "     MAE: 0.8294, R²: 0.5242\n",
      "  ➤ Layer 3\n",
      "     MAE: 0.8176, R²: 0.5338\n",
      "  ➤ Layer 4\n",
      "     MAE: 0.7651, R²: 0.5841\n",
      "  ➤ Layer 5\n",
      "     MAE: 0.7553, R²: 0.5943\n",
      "  ➤ Layer 6\n",
      "     MAE: 0.7141, R²: 0.6296\n",
      "  ➤ Layer 7\n",
      "     MAE: 0.7095, R²: 0.6353\n",
      "  ➤ Layer 8\n",
      "     MAE: 0.7052, R²: 0.6391\n",
      "  ➤ Layer 9\n",
      "     MAE: 0.7039, R²: 0.6400\n",
      "  ➤ Layer 10\n",
      "     MAE: 0.7119, R²: 0.6329\n",
      "  ➤ Layer 11\n",
      "     MAE: 0.7304, R²: 0.6165\n",
      "  ➤ Layer 12\n",
      "     MAE: 0.7640, R²: 0.5929\n",
      "\n",
      "Fold 3/5 - Number of Syllables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [07:55<00:00, 201.96it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:29<00:00, 201.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 1.3687, R²: -0.0001\n",
      "  ➤ Layer 1\n",
      "     MAE: 0.8830, R²: 0.4738\n",
      "  ➤ Layer 2\n",
      "     MAE: 0.8365, R²: 0.5227\n",
      "  ➤ Layer 3\n",
      "     MAE: 0.8223, R²: 0.5348\n",
      "  ➤ Layer 4\n",
      "     MAE: 0.7684, R²: 0.5834\n",
      "  ➤ Layer 5\n",
      "     MAE: 0.7562, R²: 0.5965\n",
      "  ➤ Layer 6\n",
      "     MAE: 0.7208, R²: 0.6257\n",
      "  ➤ Layer 7\n",
      "     MAE: 0.7119, R²: 0.6347\n",
      "  ➤ Layer 8\n",
      "     MAE: 0.7098, R²: 0.6362\n",
      "  ➤ Layer 9\n",
      "     MAE: 0.7094, R²: 0.6368\n",
      "  ➤ Layer 10\n",
      "     MAE: 0.7157, R²: 0.6315\n",
      "  ➤ Layer 11\n",
      "     MAE: 0.7355, R²: 0.6158\n",
      "  ➤ Layer 12\n",
      "     MAE: 0.7719, R²: 0.5908\n",
      "\n",
      "Fold 4/5 - Number of Syllables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [07:54<00:00, 202.37it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:29<00:00, 201.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 1.3723, R²: -0.0000\n",
      "  ➤ Layer 1\n",
      "     MAE: 0.8762, R²: 0.4765\n",
      "  ➤ Layer 2\n",
      "     MAE: 0.8283, R²: 0.5300\n",
      "  ➤ Layer 3\n",
      "     MAE: 0.8159, R²: 0.5401\n",
      "  ➤ Layer 4\n",
      "     MAE: 0.7644, R²: 0.5879\n",
      "  ➤ Layer 5\n",
      "     MAE: 0.7524, R²: 0.6011\n",
      "  ➤ Layer 6\n",
      "     MAE: 0.7134, R²: 0.6347\n",
      "  ➤ Layer 7\n",
      "     MAE: 0.7052, R²: 0.6429\n",
      "  ➤ Layer 8\n",
      "     MAE: 0.7011, R²: 0.6458\n",
      "  ➤ Layer 9\n",
      "     MAE: 0.7034, R²: 0.6441\n",
      "  ➤ Layer 10\n",
      "     MAE: 0.7085, R²: 0.6380\n",
      "  ➤ Layer 11\n",
      "     MAE: 0.7259, R²: 0.6244\n",
      "  ➤ Layer 12\n",
      "     MAE: 0.7640, R²: 0.5972\n",
      "\n",
      "Fold 5/5 - Number of Syllables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Embeddings:   0%|          | 0/96000 [00:00<?, ?it/s]/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Train Embeddings: 100%|██████████| 96000/96000 [07:55<00:00, 201.74it/s]\n",
      "Test Embeddings: 100%|██████████| 30000/30000 [02:28<00:00, 201.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➤ Layer 0\n",
      "     MAE: 1.3628, R²: -0.0001\n",
      "  ➤ Layer 1\n",
      "     MAE: 0.8667, R²: 0.4837\n",
      "  ➤ Layer 2\n",
      "     MAE: 0.8201, R²: 0.5339\n",
      "  ➤ Layer 3\n",
      "     MAE: 0.8092, R²: 0.5446\n",
      "  ➤ Layer 4\n",
      "     MAE: 0.7574, R²: 0.5934\n",
      "  ➤ Layer 5\n",
      "     MAE: 0.7485, R²: 0.6044\n",
      "  ➤ Layer 6\n",
      "     MAE: 0.7109, R²: 0.6355\n",
      "  ➤ Layer 7\n",
      "     MAE: 0.7048, R²: 0.6430\n",
      "  ➤ Layer 8\n",
      "     MAE: 0.7002, R²: 0.6472\n",
      "  ➤ Layer 9\n",
      "     MAE: 0.6989, R²: 0.6475\n",
      "  ➤ Layer 10\n",
      "     MAE: 0.7050, R²: 0.6422\n",
      "  ➤ Layer 11\n",
      "     MAE: 0.7208, R²: 0.6276\n",
      "  ➤ Layer 12\n",
      "     MAE: 0.7578, R²: 0.6019\n"
     ]
    }
   ],
   "source": [
    "results_nsyl = probing_cv(words, nsyl.tolist(), \"Number of Syllables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec4f7507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(results, feature_name):\n",
    "    records = []\n",
    "    for layer, scores in results.items():\n",
    "        for fold in range(len(scores['mae'])):\n",
    "            records.append({\n",
    "                'feature': feature_name,\n",
    "                'layer': layer,\n",
    "                'fold': fold,\n",
    "                'mae': scores['mae'][fold],\n",
    "                'r2': scores['r2'][fold]\n",
    "            })\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(f\"{EMBEDDING_DIR}/{feature_name.lower()}_probing_results.csv\", index=False)\n",
    "\n",
    "save_results_to_csv(results_img, \"Imageability\")\n",
    "save_results_to_csv(results_conc, \"Concreteness\")\n",
    "save_results_to_csv(results_nsyl, \"Number of Syllables\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
