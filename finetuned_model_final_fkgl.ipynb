{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f37ded88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textstat in ./.venv/lib/python3.12/site-packages (0.7.8)\n",
      "Requirement already satisfied: pyphen in ./.venv/lib/python3.12/site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: cmudict in ./.venv/lib/python3.12/site-packages (from textstat) (1.1.1)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from textstat) (80.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=5 in ./.venv/lib/python3.12/site-packages (from cmudict->textstat) (8.7.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in ./.venv/lib/python3.12/site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.12/site-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0bd96e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharmajidotdev/manish/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing the Readability Dataset\n",
      "Loading and processing documents from the corpus...\n",
      "Successfully processed 567 documents.\n",
      "Example document text: 'Intermediate \n",
      "Water scientists have given one of the strongest warnings ever about global food suppl...'\n",
      "Example FKGL score: 11.94\n",
      "\n",
      "Step 1: Completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "print(\"Step 1: Preparing the Readability Dataset\")\n",
    "\n",
    "corpus_path = \"/home/sharmajidotdev/manish/msdataset/Texts-SeparatedByReadingLevel\" \n",
    "\n",
    "if not os.path.exists(corpus_path):\n",
    "    print(f\"Error: The corpus directory '{corpus_path}' does not exist.\")\n",
    "    print(\"Please download the corpus \")\n",
    "    exit()\n",
    "\n",
    "all_documents = []\n",
    "all_fkgl_scores = []\n",
    "all_fre_scores = []\n",
    "all_levels = []\n",
    "\n",
    "print(\"Loading and processing documents from the corpus...\")\n",
    "for level_dir in os.listdir(corpus_path):\n",
    "    level_path = os.path.join(corpus_path, level_dir)\n",
    "    if os.path.isdir(level_path):\n",
    "        for filename in os.listdir(level_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(level_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    \n",
    "                    if text.strip():\n",
    "                        fkgl = textstat.flesch_kincaid_grade(text)\n",
    "                        fre = textstat.flesch_reading_ease(text)\n",
    "                        \n",
    "                        all_documents.append(text)\n",
    "                        all_fkgl_scores.append(fkgl)\n",
    "                        all_fre_scores.append(fre)\n",
    "                        all_levels.append(level_dir)\n",
    "\n",
    "readability_dataset = Dataset.from_dict({\n",
    "    'text': all_documents,\n",
    "    'fkgl_score': all_fkgl_scores,\n",
    "    'fre_score': all_fre_scores,\n",
    "    'level': all_levels\n",
    "})\n",
    "    \n",
    "print(f\"Successfully processed {len(readability_dataset)} documents.\")\n",
    "print(f\"Example document text: '{readability_dataset[0]['text'][:100]}...'\")\n",
    "print(f\"Example FKGL score: {readability_dataset[0]['fkgl_score']:.2f}\")\n",
    "\n",
    "print(\"\\nStep 1: Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff8481b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step 2: Fine-Tuning BERT for FKGL Scores \n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Step 2: Fine-Tuning BERT for FKGL Scores \")\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "MODEL_DIR_READABILITY = \"/home/sharmajidotdev/manish/models/readability\"\n",
    "os.makedirs(MODEL_DIR_READABILITY, exist_ok=True)\n",
    "os.makedirs(os.path.join(MODEL_DIR_READABILITY, \"Fkgl\"), exist_ok=True)\n",
    "\n",
    "fine_tune_results_fkgl = {'mae': [], 'r2': []}\n",
    "\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "def tokenize_and_label_function(examples, label_column_name):\n",
    "    tokenized_inputs = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    tokenized_inputs['labels'] = torch.tensor(examples[label_column_name], dtype=torch.float32)\n",
    "    return tokenized_inputs\n",
    "\n",
    "def compute_metrics_regression(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    mae = mean_absolute_error(labels, predictions)\n",
    "    r2 = r2_score(labels, predictions)\n",
    "    return {\"mae\": mae, \"r2\": r2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f24b7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/5 already completed. Skipping. ---\n",
      "\n",
      " Fine Tuning for Fold 2/5 \n",
      "\n",
      " Fine Tuning for FKGL for Fold 2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 362/362 [00:00<00:00, 1612.02 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 1540.72 examples/s]\n",
      "Map: 100%|██████████| 114/114 [00:00<00:00, 1235.22 examples/s]\n",
      "/tmp/ipykernel_10404/2477695128.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_fkgl = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [138/138 00:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21.466900</td>\n",
       "      <td>11.020518</td>\n",
       "      <td>2.681038</td>\n",
       "      <td>-1.398791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.778700</td>\n",
       "      <td>4.518609</td>\n",
       "      <td>1.778632</td>\n",
       "      <td>0.016453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.035700</td>\n",
       "      <td>3.883676</td>\n",
       "      <td>1.657008</td>\n",
       "      <td>0.154656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 FKGL Test Results: {'eval_loss': 3.4213476181030273, 'eval_mae': 1.5118625164031982, 'eval_r2': 0.13016116619110107, 'eval_runtime': 1.4451, 'eval_samples_per_second': 78.888, 'eval_steps_per_second': 10.38, 'epoch': 3.0}\n",
      "\n",
      " Fine Tuning for Fold 3/5 \n",
      "\n",
      " Fine Tuning for FKGL for Fold 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 363/363 [00:00<00:00, 1751.22 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 1610.03 examples/s]\n",
      "Map: 100%|██████████| 113/113 [00:00<00:00, 1447.24 examples/s]\n",
      "/tmp/ipykernel_10404/2477695128.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_fkgl = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [138/138 00:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.233200</td>\n",
       "      <td>11.597965</td>\n",
       "      <td>2.842669</td>\n",
       "      <td>-1.699969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.054800</td>\n",
       "      <td>4.232935</td>\n",
       "      <td>1.680344</td>\n",
       "      <td>0.014586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.003100</td>\n",
       "      <td>2.687883</td>\n",
       "      <td>1.274172</td>\n",
       "      <td>0.374270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 FKGL Test Results: {'eval_loss': 2.9014599323272705, 'eval_mae': 1.3084485530853271, 'eval_r2': 0.3188062906265259, 'eval_runtime': 1.1583, 'eval_samples_per_second': 97.556, 'eval_steps_per_second': 12.95, 'epoch': 3.0}\n",
      "\n",
      " Fine Tuning for Fold 4/5 \n",
      "\n",
      " Fine Tuning for FKGL for Fold 4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 363/363 [00:00<00:00, 2284.77 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 2053.23 examples/s]\n",
      "Map: 100%|██████████| 113/113 [00:00<00:00, 2055.48 examples/s]\n",
      "/tmp/ipykernel_10404/2477695128.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_fkgl = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [138/138 00:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>22.797800</td>\n",
       "      <td>12.535304</td>\n",
       "      <td>2.949249</td>\n",
       "      <td>-1.656501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.930000</td>\n",
       "      <td>4.679214</td>\n",
       "      <td>1.725635</td>\n",
       "      <td>0.008374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.834900</td>\n",
       "      <td>3.645015</td>\n",
       "      <td>1.489598</td>\n",
       "      <td>0.227543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 FKGL Test Results: {'eval_loss': 3.1573281288146973, 'eval_mae': 1.406311273574829, 'eval_r2': 0.2824860215187073, 'eval_runtime': 1.1876, 'eval_samples_per_second': 95.147, 'eval_steps_per_second': 12.63, 'epoch': 3.0}\n",
      "\n",
      " Fine Tuning for Fold 5/5 \n",
      "\n",
      " Fine Tuning for FKGL for Fold 5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 363/363 [00:00<00:00, 2050.81 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 2162.11 examples/s]\n",
      "Map: 100%|██████████| 113/113 [00:00<00:00, 2200.86 examples/s]\n",
      "/tmp/ipykernel_10404/2477695128.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_fkgl = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [138/138 00:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19.673900</td>\n",
       "      <td>12.253371</td>\n",
       "      <td>2.860460</td>\n",
       "      <td>-1.382176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.976200</td>\n",
       "      <td>4.997544</td>\n",
       "      <td>1.841723</td>\n",
       "      <td>0.028428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.410800</td>\n",
       "      <td>3.350609</td>\n",
       "      <td>1.490520</td>\n",
       "      <td>0.348609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 FKGL Test Results: {'eval_loss': 2.42488431930542, 'eval_mae': 1.1937713623046875, 'eval_r2': 0.38742566108703613, 'eval_runtime': 1.1683, 'eval_samples_per_second': 96.72, 'eval_steps_per_second': 12.839, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_val_indices, test_indices) in enumerate(kf.split(readability_dataset)):\n",
    "    \n",
    "    fkgl_checkpoint_path = os.path.join(MODEL_DIR_READABILITY, f\"Fkgl/best_fold_{fold_idx}\")\n",
    "    \n",
    "    if os.path.exists(fkgl_checkpoint_path):\n",
    "        print(f\"\\n--- Fold {fold_idx + 1}/{N_SPLITS} already completed. Skipping. ---\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n Fine Tuning for Fold {fold_idx + 1}/{N_SPLITS} \")\n",
    "\n",
    "    train_val_dataset = readability_dataset.select(train_val_indices)\n",
    "    test_dataset = readability_dataset.select(test_indices)\n",
    "    \n",
    "    train_dataset, val_dataset = train_val_dataset.train_test_split(test_size=0.2).values()\n",
    "    \n",
    "    print(f\"\\n Fine Tuning for FKGL for Fold {fold_idx + 1} \")\n",
    "    model_fkgl = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device)\n",
    "\n",
    "    tokenized_train_fkgl = train_dataset.map(lambda x: tokenize_and_label_function(x, 'fkgl_score'), batched=True)\n",
    "    tokenized_val_fkgl = val_dataset.map(lambda x: tokenize_and_label_function(x, 'fkgl_score'), batched=True)\n",
    "    tokenized_test_fkgl = test_dataset.map(lambda x: tokenize_and_label_function(x, 'fkgl_score'), batched=True)\n",
    "    \n",
    "    training_args_fkgl = TrainingArguments(\n",
    "        output_dir=os.path.join(MODEL_DIR_READABILITY, f\"Fkgl/fold_{fold_idx}\"),\n",
    "        run_name=f\"fkgl-fold-{fold_idx}\",\n",
    "        num_train_epochs=3, per_device_train_batch_size=8, per_device_eval_batch_size=8,logging_steps=10,\n",
    "        weight_decay=0.01, eval_strategy=\"epoch\", logging_dir='./logs',\n",
    "        save_strategy=\"epoch\", load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer_fkgl = Trainer(\n",
    "        model=model_fkgl, args=training_args_fkgl, train_dataset=tokenized_train_fkgl,\n",
    "        eval_dataset=tokenized_val_fkgl, compute_metrics=compute_metrics_regression, tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer_fkgl.train()\n",
    "    eval_results_fkgl = trainer_fkgl.evaluate(tokenized_test_fkgl)\n",
    "    fine_tune_results_fkgl['mae'].append(eval_results_fkgl['eval_mae'])\n",
    "    fine_tune_results_fkgl['r2'].append(eval_results_fkgl['eval_r2'])\n",
    "    print(f\"Fold {fold_idx+1} FKGL Test Results: {eval_results_fkgl}\")\n",
    "    model_fkgl.save_pretrained(os.path.join(MODEL_DIR_READABILITY, f\"Fkgl/best_fold_{fold_idx}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91fca095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " FKGL Fine Tuning Summary \n",
      "Average Fine Tuning Performance (FKGL) across 5 folds: MAE=1.3551, R2=0.2797\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n FKGL Fine Tuning Summary \")\n",
    "avg_fkgl_mae = np.mean(fine_tune_results_fkgl['mae'])\n",
    "avg_fkgl_r2 = np.mean(fine_tune_results_fkgl['r2'])\n",
    "print(f\"Average Fine Tuning Performance (FKGL) across 5 folds: MAE={avg_fkgl_mae:.4f}, R2={avg_fkgl_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a6d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
